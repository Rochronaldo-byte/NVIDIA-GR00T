GR00T N1 features a dual-system architecture inspired by human cognition, consisting of the following complementary components:

Vision-Language Model (System 2): This methodical thinking system is based on NVIDIA-Eagle with SmolLM-1.7B. It interprets the environment through vision and language instructions, enabling robots to reason about their environment and instructions, and plan the right actions.
Diffusion Transformer (System 1): This action model generates continuous actions to control the robotâ€™s movements, translating the action plan made by System 2 into precise, continuous robot movements. 

Training a generalist model like GR00T N1 demands a robust data approach that leverages the complementary benefits of diverse data types. The GR00T N1 training data forms a pyramid, with data quantity decreasing and embodiment specificity increasing from base to peak.

At the foundation, internet-scale web data and human videos provide a broad base of visual and linguistic information. These datasets capture human-object interactions, offering insights into natural motion patterns and task semantics. 
The middle layer incorporates synthetic data generated by the NVIDIA Omniverse platform. 
At the peak is real robot data collected through teleoperation on various platforms, offering precise insights into robotic capabilities.
Human-centered online videos provide valuable insights into human-object interactions but lack motor control signals for robots. Simulation data fills this gap with infinite, real-time data through GPU acceleration, though it faces a simulation-to-reality gap. 

Real robot data bridges this gap but is costly and time-consuming. By combining this diverse data and using techniques such as latent action training, which teaches robots to learn from large-scale, unlabeled, human video data without supervision, a robust strategy emerges that enhances robot training, improving the performance and adaptability of GR00T N1.

This approach was put into practice using the NVIDIA Isaac GR00T blueprint. With it, over 750K synthetic trajectories were generated in just 11 hours, equivalent to 6.5K hours or nine continuous months of human demonstration data. The integration of this synthetic data with real data resulted in a 40% performance boost for GR00T N1 compared to using only real data.

Hands-on with GR00T N1
You can get started with GR00T N1 using the following steps:

Data preparation: Format your robot demonstration data (video, state, action) triplets into a GR00T dataset, which is compatible with the Hugging Face LeRobot format. 
Data validation: Use the validation script to ensure that your data adheres to the correct format. 
Post-training: Use PyTorch scripts to fine-tune the pretrained GR00T N1 model with your custom dataset. 
Inference: Connect the inference script to your robot controller to execute the actions on your target hardware or your simulation environment using the post-trained GR00T N1 model.
Evaluation: Run the evaluation scripts to get the task-success rate of the model.
Performance
The GR00T N1 models were evaluated using both simulated and real-world benchmarks to assess their performance in diverse robotic embodiments and manipulation tasks. Simulation experiments used three distinct benchmarks, while real-world tests focused on tabletop manipulation tasks with the GR-1 humanoid robot. 

Simulation benchmarks
Three benchmarks are used for simulation experiments: two open-source ones from prior studies and a new suite mirroring real-world tabletop manipulation tasks, chosen to evaluate the models across different robot embodiments and diverse manipulation tasks.

